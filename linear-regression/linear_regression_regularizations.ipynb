{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('housing_dataset/housing_X_train.csv', header=None).values.T\n",
    "y_train = pd.read_csv('housing_dataset/housing_y_train.csv', header=None).values.ravel()\n",
    "X_test = pd.read_csv('housing_dataset/housing_X_test.csv', header=None).values.T\n",
    "y_test = pd.read_csv('housing_dataset/housing_y_test.csv', header=None).values.ravel()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_norm = sc.fit_transform(X_train)\n",
    "X_test_norm = sc.transform(X_test)\n",
    "\n",
    "# def normalize(X):\n",
    "#     mean = np.mean(X, axis=0)\n",
    "#     std = np.std(X, axis=0)\n",
    "#     return (X - mean) / std\n",
    "\n",
    "# X_train_norm = normalize(X_train)\n",
    "# X_test_norm = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression algorithm using the closed form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0, Training MSE: 662.1461594616705, Test MSE: 576.25\n",
      "Lambda: 0.25, Training MSE: 662.1462363552637, Test MSE: 575.39\n",
      "Lambda: 0.5, Training MSE: 662.1464647289267, Test MSE: 574.55\n",
      "Lambda: 0.75, Training MSE: 662.1468411939754, Test MSE: 573.75\n",
      "Lambda: 1, Training MSE: 662.1473624551902, Test MSE: 572.96\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression(X, y, lambda_):\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    I = np.identity(n_features)\n",
    "    \n",
    "    w = np.linalg.solve(X.T @ X + lambda_ * I, X.T @ y)\n",
    "    return w\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Squared Error.\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "lambdas = [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    weights = ridge_regression(X_train_norm, y_train, lambda_)\n",
    "    \n",
    "    # Predict on the training set\n",
    "    y_train_pred = X_train_norm @ weights\n",
    "    train_mse = compute_mse(y_train, y_train_pred)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_test_pred = X_test_norm @ weights\n",
    "    test_mse = compute_mse(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Lambda: {lambda_}, Training MSE: {train_mse}, Test MSE: {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression using the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66252605  0.35681172  0.09909461  0.21276072 -0.56303703  6.13476605\n",
      " -1.10171694 -1.75664123  0.17276305 -1.04377504 -1.36794469  0.71894249\n",
      " -1.12162724]\n",
      "Lambda: 0, Training MSE: 662.23, Test MSE: 563.22\n",
      "[ 0.26755566  0.38471732 -0.23143633  0.27400512 -0.23076865  4.58775408\n",
      " -0.55088491 -1.2175447   0.17141993 -0.88662828 -1.37975465  0.58743096\n",
      " -1.71966582]\n",
      "Lambda: 0.25, Training MSE: 664.26, Test MSE: 585.79\n",
      "[ 0.11485842  0.39428857 -0.35905161  0.28141285 -0.15857259  3.79672506\n",
      " -0.39567312 -0.93667734  0.17309963 -0.78274482 -1.31900103  0.50950701\n",
      " -1.7668437 ]\n",
      "Lambda: 0.5, Training MSE: 667.03, Test MSE: 610.77\n",
      "[ 0.03635506  0.3966007  -0.415408    0.27276699 -0.14420778  3.28642842\n",
      " -0.33864083 -0.75396009  0.17023149 -0.70719712 -1.24097225  0.45612618\n",
      " -1.69754911]\n",
      "Lambda: 0.75, Training MSE: 669.84, Test MSE: 620.40\n",
      "[-0.01119932  0.39460861 -0.44069561  0.25901849 -0.14501816  2.91693149\n",
      " -0.31266734 -0.62279004  0.1643112  -0.64867449 -1.1626942   0.41643266\n",
      " -1.60439012]\n",
      "Lambda: 1, Training MSE: 672.52, Test MSE: 622.72\n"
     ]
    }
   ],
   "source": [
    "def ridge_regression_gradient_descent(X, y, lambda_, learning_rate=0.1, n_iterations=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        y_pred = X @ w\n",
    "        gradient = (X.T @ (y_pred - y)) / n_samples + lambda_ * w\n",
    "\n",
    "        w -= learning_rate * gradient\n",
    "    \n",
    "    return w\n",
    "\n",
    "lambdas = [0, 0.25, 0.5, 0.75, 1]\n",
    "learning_rate = 0.01\n",
    "n_epochs = 1000\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    weights = ridge_regression_gradient_descent(X_train_norm, y_train, lambda_, learning_rate, n_epochs)\n",
    "    # print(weights)\n",
    "    # Predict on the training set\n",
    "    y_train_pred = X_train_norm @ weights\n",
    "    train_mse = compute_mse(y_train, y_train_pred)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_test_pred = X_test_norm @ weights\n",
    "    test_mse = compute_mse(y_test, y_test_pred)\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Lambda: {lambda_}, Training MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66252605  0.35681172  0.09909461  0.21276072 -0.56303703  6.13476605\n",
      " -1.10171694 -1.75664123  0.17276305 -1.04377504 -1.36794469  0.71894249\n",
      " -1.12162724]\n",
      "Lambda: 0, Training MSE: 662.23, Test MSE: 563.22\n",
      "[ 3.38669477e-03  3.59870653e-03 -2.74228251e-03  5.13639841e-02\n",
      " -5.51339923e-04  6.16867548e+00 -5.04050030e-01 -7.11446292e-01\n",
      " -7.47973686e-04 -7.02301054e-01 -1.31000888e+00  3.52037469e-01\n",
      " -1.09402803e+00]\n",
      "Lambda: 0.25, Training MSE: 663.12, Test MSE: 741.39\n",
      "[ 1.63323814e-03  1.45121636e-03 -1.79798684e-03  3.25233421e-03\n",
      "  3.63504841e-03  6.15748816e+00 -6.15297151e-03 -1.31064158e-02\n",
      "  1.46601498e-03 -5.05093179e-01 -1.11007863e+00  1.00149240e-01\n",
      " -1.02584322e+00]\n",
      "Lambda: 0.5, Training MSE: 664.58, Test MSE: 685.37\n",
      "[ 4.05270942e-04 -2.71636698e-03  4.21528233e-03  6.67658930e-03\n",
      "  1.41927497e-03  6.12293634e+00 -1.05533836e-02 -1.00715095e-02\n",
      " -2.04961961e-03 -3.18211766e-01 -9.00919659e-01  1.08463024e-02\n",
      " -9.03076318e-01]\n",
      "Lambda: 0.75, Training MSE: 665.37, Test MSE: 633.81\n",
      "[ 6.51002287e-03  1.22540300e-02 -2.74633202e-03  8.55373562e-03\n",
      " -1.54719783e-03  6.05360416e+00  1.38165312e-03  6.71545166e-03\n",
      "  2.14210771e-03 -1.15765099e-01 -7.07379707e-01  1.04524283e-02\n",
      " -7.96802282e-01]\n",
      "Lambda: 1, Training MSE: 666.38, Test MSE: 582.13\n"
     ]
    }
   ],
   "source": [
    "def lasso_regression_gradient_descent(X, y, lambda_, learning_rate=0.1, n_epochs=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        y_pred = X @ w\n",
    "        gradient = (X.T @ (y_pred - y)) / n_samples\n",
    "        \n",
    "        # lasso gradient with subgradient for L1 regularization\n",
    "        w_sign = np.sign(w)\n",
    "        gradient += lambda_ * w_sign\n",
    "        \n",
    "        w -= learning_rate * gradient\n",
    "\n",
    "    return w\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    weights = lasso_regression_gradient_descent(X_train_norm, y_train, lambda_, learning_rate, n_epochs)\n",
    "    # print(weights)\n",
    "    y_train_pred = X_train_norm @ weights\n",
    "    train_mse = compute_mse(y_train, y_train_pred)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_test_pred = X_test_norm @ weights\n",
    "    test_mse = compute_mse(y_test, y_test_pred)\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Lambda: {lambda_}, Training MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
