{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = 'dataset/train-images.idx3-ubyte'\n",
    "train_labels_path = 'dataset/train-labels.idx1-ubyte'\n",
    "test_images_path = 'dataset/t10k-images.idx3-ubyte'\n",
    "test_labels_path = 'dataset/t10k-labels.idx1-ubyte'\n",
    "\n",
    "def load_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "X_train = load_idx(train_images_path).reshape(60000, 784)\n",
    "y_train = load_idx(train_labels_path)\n",
    "X_test = load_idx(test_images_path).reshape(10000, 784)\n",
    "y_test = load_idx(test_labels_path)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_pca = sc.fit_transform(X_train_pca)\n",
    "X_test_pca = sc.transform(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 20)\n",
      "X_test shape: (10000, 20)\n",
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train_pca.shape)\n",
    "print('X_test shape:', X_test_pca.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21485558, -0.63392669, -0.05330357, ...,  1.4788542 ,\n",
       "         1.38552912,  0.81848814],\n",
       "       [ 1.75396433, -0.59780343,  1.296685  , ...,  0.1904717 ,\n",
       "         0.26409796, -0.52068984],\n",
       "       [-0.08988901,  0.7951055 , -0.40989698, ..., -0.58292729,\n",
       "        -0.04552189, -2.19985469],\n",
       "       ...,\n",
       "       [-0.30868215,  0.32454813, -0.56015577, ..., -0.17654919,\n",
       "         0.52106816,  1.17603901],\n",
       "       [ 0.22642506, -0.01133729,  1.11733798, ..., -0.675643  ,\n",
       "        -0.38153217,  0.98985009],\n",
       "       [-0.30067703, -0.05011575,  1.20901145, ...,  1.28963031,\n",
       "         0.48043359, -1.03866323]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.02437606 -0.49148246  0.14507152 -0.21892737 -1.99115848  0.54067845\n",
      "   0.26848735 -0.78103489  0.32132791  0.74703492 -0.30804126  0.51787238\n",
      "  -0.28139649  0.17177089  0.44335001  0.01631751  0.28366148 -0.11129851\n",
      "   0.23263761 -0.31363938]\n",
      " [ 0.94034654 -0.40714981 -0.19704464 -0.18651383 -0.04433035 -0.3694063\n",
      "  -0.09271826  0.07401276  0.26528453 -0.54436722  0.15430353 -0.43640322\n",
      "  -0.28380789 -0.33495419 -0.06661999 -0.08351695 -0.06578946  0.02744067\n",
      "  -0.49125219  0.49683461]\n",
      " [-0.77331498  0.94010884 -0.05878871 -0.47965368 -0.0409193   0.15337332\n",
      "   0.24957371 -0.12728275 -0.26043702 -0.28752336 -0.53537827  0.10265759\n",
      "  -0.58524666 -0.24768054  0.59051776 -0.27507232  0.12821122  0.05479359\n",
      "   0.13330785  0.15129254]\n",
      " [ 0.36096793 -0.58948847  0.7406867   1.17462259  0.88813236  0.9644507\n",
      "   0.33173513  0.74662143 -0.84649634  0.55517848  0.30719582  0.04344685\n",
      "  -0.70681113 -0.30147136  0.65986934  0.05799599 -0.40952841 -0.04957824\n",
      "   0.01290013 -0.30615894]\n",
      " [ 0.62613507  0.29835352  0.36716166  1.73463367  0.59179594 -1.10209133\n",
      "   0.6499125  -0.827605   -0.34022181 -0.856283   -0.83298275  0.18588942\n",
      "   0.60058623  0.57720966  0.08562891 -0.35099747 -0.02121594  0.57718247\n",
      "   0.41465764  0.08739038]\n",
      " [ 0.16581744  1.44338806 -0.2220173   0.15644494  0.01595244  0.23792186\n",
      "  -0.23778455  0.37888598  0.2794178   0.04565085  0.20073673  0.01627655\n",
      "   0.18947344  0.18853146 -0.53439672  0.09571682 -0.06938446 -0.31602627\n",
      "   0.02316953 -0.09846486]\n",
      " [-0.28748318 -0.34724829  0.13413218 -0.2037551   0.29006195 -0.10321757\n",
      "  -0.22583214 -0.1934419  -0.01969104  0.29557828  0.14554707 -0.14035263\n",
      "   0.45184908 -0.16194778  0.17586356  0.27289526  0.28000141  0.20994845\n",
      "   0.04647701 -0.021641  ]\n",
      " [-1.56388765 -1.17412341  0.36301373  0.29369829 -0.51035608  0.73409035\n",
      "  -0.18015112 -0.29064818  0.97473991 -0.01589891 -0.56106232 -0.48938041\n",
      "  -0.28848788  0.95387332 -1.17077203  0.20130623 -0.01323718 -0.0248768\n",
      "  -0.3073861   0.17689797]\n",
      " [-1.50610205 -0.52574167 -0.1165077   0.12690392 -0.96211048 -0.81997824\n",
      "   0.88625818 -0.70616661 -0.33847311  0.41854574  1.08960173  0.86052195\n",
      "   0.11277239 -0.15740464 -0.52690983 -0.60227001 -0.74217991 -0.64503678\n",
      "  -0.15565167 -0.5043828 ]\n",
      " [ 0.36177855 -1.30787747 -0.83227169 -0.77760565  0.3451322  -0.23917152\n",
      "  -0.23632419  1.34798963 -0.4441544  -0.5418159  -0.20343024  0.45562431\n",
      "  -0.13384636  0.50204207 -0.13877978 -0.2649319  -0.46383813 -0.05808551\n",
      "   0.36152147 -0.28531949]] [0.0507104  0.12897137 0.12429362 0.05885036 0.04643138 0.16313619\n",
      " 0.27319102 0.05002512 0.04613656 0.05825399] [[[0.49103056 0.20431315 1.18482354 ... 0.85205777 1.40403967 0.88047834]\n",
      "  [0.49103056 0.20431315 1.18482354 ... 0.85205777 1.40403967 0.88047834]\n",
      "  [0.49103056 0.20431315 1.18482354 ... 0.85205777 1.40403967 0.88047834]\n",
      "  ...\n",
      "  [0.49103056 0.20431315 1.18482354 ... 0.85205777 1.40403967 0.88047834]\n",
      "  [0.49103056 0.20431315 1.18482354 ... 0.85205777 1.40403967 0.88047834]\n",
      "  [0.49103056 0.20431315 1.18482354 ... 0.85205777 1.40403967 0.88047834]]\n",
      "\n",
      " [[0.70152913 0.29585349 2.09939248 ... 1.40474748 1.11825588 1.47086722]\n",
      "  [0.70152913 0.29585349 2.09939248 ... 1.40474748 1.11825588 1.47086722]\n",
      "  [0.70152913 0.29585349 2.09939248 ... 1.40474748 1.11825588 1.47086722]\n",
      "  ...\n",
      "  [0.70152913 0.29585349 2.09939248 ... 1.40474748 1.11825588 1.47086722]\n",
      "  [0.70152913 0.29585349 2.09939248 ... 1.40474748 1.11825588 1.47086722]\n",
      "  [0.70152913 0.29585349 2.09939248 ... 1.40474748 1.11825588 1.47086722]]\n",
      "\n",
      " [[0.09545911 0.27877066 0.46130266 ... 0.4342766  0.77706539 0.86423317]\n",
      "  [0.09545911 0.27877066 0.46130266 ... 0.4342766  0.77706539 0.86423317]\n",
      "  [0.09545911 0.27877066 0.46130266 ... 0.4342766  0.77706539 0.86423317]\n",
      "  ...\n",
      "  [0.09545911 0.27877066 0.46130266 ... 0.4342766  0.77706539 0.86423317]\n",
      "  [0.09545911 0.27877066 0.46130266 ... 0.4342766  0.77706539 0.86423317]\n",
      "  [0.09545911 0.27877066 0.46130266 ... 0.4342766  0.77706539 0.86423317]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.01643789 0.07739781 0.06791509 ... 0.34755881 0.21499085 0.33147544]\n",
      "  [0.01643789 0.07739781 0.06791509 ... 0.34755881 0.21499085 0.33147544]\n",
      "  [0.01643789 0.07739781 0.06791509 ... 0.34755881 0.21499085 0.33147544]\n",
      "  ...\n",
      "  [0.01643789 0.07739781 0.06791509 ... 0.34755881 0.21499085 0.33147544]\n",
      "  [0.01643789 0.07739781 0.06791509 ... 0.34755881 0.21499085 0.33147544]\n",
      "  [0.01643789 0.07739781 0.06791509 ... 0.34755881 0.21499085 0.33147544]]\n",
      "\n",
      " [[0.02360829 0.07126124 0.06002209 ... 0.17349721 0.07594152 0.18236239]\n",
      "  [0.02360829 0.07126124 0.06002209 ... 0.17349721 0.07594152 0.18236239]\n",
      "  [0.02360829 0.07126124 0.06002209 ... 0.17349721 0.07594152 0.18236239]\n",
      "  ...\n",
      "  [0.02360829 0.07126124 0.06002209 ... 0.17349721 0.07594152 0.18236239]\n",
      "  [0.02360829 0.07126124 0.06002209 ... 0.17349721 0.07594152 0.18236239]\n",
      "  [0.02360829 0.07126124 0.06002209 ... 0.17349721 0.07594152 0.18236239]]\n",
      "\n",
      " [[0.31735802 0.18672652 0.75464603 ... 1.23258942 1.03230331 0.85851551]\n",
      "  [0.31735802 0.18672652 0.75464603 ... 1.23258942 1.03230331 0.85851551]\n",
      "  [0.31735802 0.18672652 0.75464603 ... 1.23258942 1.03230331 0.85851551]\n",
      "  ...\n",
      "  [0.31735802 0.18672652 0.75464603 ... 1.23258942 1.03230331 0.85851551]\n",
      "  [0.31735802 0.18672652 0.75464603 ... 1.23258942 1.03230331 0.85851551]\n",
      "  [0.31735802 0.18672652 0.75464603 ... 1.23258942 1.03230331 0.85851551]]] [[6.42886152e-006 2.28596520e-001 6.14757723e-006 ... 3.93129445e-068\n",
      "  7.50722328e-064 1.04692384e-001]\n",
      " [9.68252691e-001 3.16059154e-002 1.98783732e-018 ... 3.05880113e-153\n",
      "  9.54484262e-116 6.61565610e-006]\n",
      " [9.46383613e-011 4.77111252e-004 3.75593527e-006 ... 6.99078762e-056\n",
      "  2.14057740e-049 6.36258132e-009]\n",
      " ...\n",
      " [1.65572123e-008 5.61054215e-003 3.23811155e-002 ... 1.93493704e-047\n",
      "  3.36374319e-037 2.25650149e-007]\n",
      " [1.07775142e-004 6.45245451e-001 1.79250939e-003 ... 1.58709663e-055\n",
      "  7.26444722e-044 2.79938907e-006]\n",
      " [2.63571478e-009 1.56893518e-002 7.00929236e-003 ... 5.61481401e-041\n",
      "  5.55060075e-040 3.26656947e-007]] [-564168.3298536773, -543679.8220066683, -529843.3618200939, -518621.3272616459, -505363.55359032383, -493503.58109188493, -486849.70912149013, -482705.1369215068, -478889.1146551251, -474687.82426082884, -470839.517864659, -467732.4510917204, -465448.7150290148, -463960.870143948, -462994.4557913665, -462380.0004620885, -461984.6896472512, -461720.38497565675, -461535.10642884276, -461400.43119154713, -461300.0387958254, -461223.7556391849, -461164.93756065465, -461119.0981092401, -461083.0993121734, -461054.666241789, -461032.097231406, -461014.09000348597, -460999.6342540598, -460987.941445898, -460978.39558649756, -460970.51653434813, -460963.9315756651, -460958.3530508812, -460953.5606943593, -460949.38770060375]\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters(X, K):\n",
    "    n, d = X.shape\n",
    "    responsibilities = np.random.rand(n, K)\n",
    "    responsibilities = responsibilities / responsibilities.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    means = X[np.random.choice(n, K, replace=False)]\n",
    "    weights = np.ones(K) / K\n",
    "    \n",
    "    # diagonal covariances as identity matrices\n",
    "    covariances = np.array([np.eye(d) for _ in range(K)])\n",
    "    \n",
    "    return responsibilities, means, weights, covariances\n",
    "\n",
    "def e_step(X, means, weights, covariances, K):\n",
    "    n, d = X.shape\n",
    "    responsibilities = np.zeros((n, K))\n",
    "    \n",
    "    for k in range(K):\n",
    "        diff = X - means[k]\n",
    "        inv_cov_diag = 1 / np.diag(covariances[k])\n",
    "        exp_term = np.exp(-0.5 * np.sum(diff ** 2 * inv_cov_diag, axis=1))\n",
    "        coef = weights[k] / np.sqrt(np.prod(np.diag(covariances[k])))\n",
    "        responsibilities[:, k] = coef * exp_term\n",
    "    \n",
    "    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "    return responsibilities\n",
    "\n",
    "def m_step(X, responsibilities, K):\n",
    "    n, d = X.shape\n",
    "    weights = responsibilities.sum(axis=0) / n\n",
    "    means = np.dot(responsibilities.T, X) / responsibilities.sum(axis=0)[:, np.newaxis]\n",
    "    \n",
    "    covariances = np.zeros((K, d, d))\n",
    "    for k in range(K):\n",
    "        diff = X - means[k]\n",
    "        weighted_diff = responsibilities[:, k, np.newaxis] * diff\n",
    "        covariances[k] = np.diag(np.dot(weighted_diff.T, diff) / responsibilities[:, k].sum())\n",
    "    \n",
    "    return means, weights, covariances\n",
    "\n",
    "def log_likelihood(X, means, weights, covariances, K):\n",
    "    n, d = X.shape\n",
    "    log_likelihoods = np.zeros(n)\n",
    "    \n",
    "    for k in range(K):\n",
    "        diff = X - means[k]\n",
    "        inv_cov_diag = 1 / np.diag(covariances[k])\n",
    "        exp_term = np.exp(-0.5 * np.sum(diff ** 2 * inv_cov_diag, axis=1))\n",
    "        coef = weights[k] / np.sqrt(np.prod(np.diag(covariances[k])))\n",
    "        log_likelihoods += coef * exp_term\n",
    "    \n",
    "    return np.sum(np.log(log_likelihoods))\n",
    "\n",
    "def em_algorithm(X, K, tol=1e-5, max_iter=500):\n",
    "    responsibilities, means, weights, covariances = initialize_parameters(X, K)\n",
    "    log_likelihoods = []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # e step, update responsibilities\n",
    "        responsibilities = e_step(X, means, weights, covariances, K)\n",
    "        \n",
    "        # m step, update parameters (means, weights, covariances)\n",
    "        means, weights, covariances = m_step(X, responsibilities, K)\n",
    "        \n",
    "        # Compute the log-likelihood\n",
    "        log_likelihood_curr = log_likelihood(X, means, weights, covariances, K)\n",
    "        log_likelihoods.append(log_likelihood_curr)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if iteration > 0 and abs(log_likelihood_curr - log_likelihoods[-2]) < tol * abs(log_likelihood_curr):\n",
    "            break\n",
    "    \n",
    "    return means, weights, covariances, responsibilities, log_likelihoods\n",
    "\n",
    "# K = 10\n",
    "# means, weights, covariances, responsibilities, log_likelihoods = em_algorithm(X_train_pca, K)\n",
    "# print(means, weights, covariances, responsibilities, log_likelihoods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GMM with K = 5\n",
      "Error rate for K = 5: 0.0908\n",
      "Training GMM with K = 10\n",
      "Error rate for K = 10: 0.0737\n",
      "Training GMM with K = 20\n",
      "Error rate for K = 20: 0.0589\n",
      "Training GMM with K = 30\n",
      "Error rate for K = 30: 0.0566\n",
      "{5: 0.0908, 10: 0.0737, 20: 0.0589, 30: 0.0566}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bayes_classifier(X_test, means_list, covariances_list, weights_list, priors, K):\n",
    "    n_test = X_test.shape[0]\n",
    "    num_classes = len(means_list)\n",
    "    \n",
    "    log_probs = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        for k in range(K):\n",
    "            diff = X_test - means_list[c][k]\n",
    "            inv_cov_diag = 1 / np.diag(covariances_list[c][k])\n",
    "            exp_term = np.exp(-0.5 * np.sum(diff ** 2 * inv_cov_diag, axis=1))\n",
    "            coef = weights_list[c][k] / np.sqrt(np.prod(np.diag(covariances_list[c][k])))\n",
    "            log_probs[:, c] += coef * exp_term\n",
    "        \n",
    "        log_probs[:, c] = np.log(log_probs[:, c] + 1e-8)  # Log to avoid underflow\n",
    "        log_probs[:, c] += np.log(priors[c]) \n",
    "    \n",
    "    return np.argmax(log_probs, axis=1)\n",
    "\n",
    "def classify_and_evaluate(X_train, y_train, X_test, y_test, K_values=[5, 10, 20, 30]):\n",
    "    num_classes = 10\n",
    "    error_rates = {}\n",
    "\n",
    "    for K in K_values:\n",
    "        print(f\"Training GMM with K = {K}\")\n",
    "        means_list, covariances_list, weights_list, priors = [], [], [], []\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            X_class = X_train[y_train == c]  # get training data for class c\n",
    "            priors.append(len(X_class) / len(X_train))  # prior\n",
    "            \n",
    "            # Fit GMM for this currentt class\n",
    "            means, weights, covariances, responsibilities, _ = em_algorithm(X_class, K)\n",
    "            means_list.append(means)\n",
    "            covariances_list.append(covariances)\n",
    "            weights_list.append(weights)\n",
    "\n",
    "        # Classify test data\n",
    "        y_pred = bayes_classifier(X_test, means_list, covariances_list, weights_list, priors, K)\n",
    "        \n",
    "        # Compute error rate\n",
    "        error_rate = np.mean(y_pred != y_test)\n",
    "        error_rates[K] = error_rate\n",
    "        print(f\"Error rate for K = {K}: {error_rate:.4f}\")\n",
    "    \n",
    "    return error_rates\n",
    "\n",
    "error_rates = classify_and_evaluate(X_train_pca, y_train, X_test_pca, y_test)\n",
    "# print(error_rates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
